### Appendix A
```
1. When taking instruction-caches into account, how does the real-world performance difference
actually look when considering optimizations focused on speed vs optimizations on code
size. I.e. can code-size optimizations sometimes outperform other types of optimizations.

2. With compilers iteratively performing different optimizations, which optimizations end up being
the most relevant, and do some optimizations actually end up frequently having negative effects?
```

### 1.1-1.3
```
1. Given that PMD's have hard requirements on how much power they can consume due to not having
access to proper cooling, is there a hard limit on the processor-performance for such devices
that we are currently approaching?

2. Why is decoding instructions made so complicated? Eg. when taking RISC-V J-Type instructions, why
is the immediate split up into 4 different sections that the decoder needs to piece together. Does
that not slow down the decoding process?
```

### 1.4-1.7
```
1. The chapter mentions testing of wafers and dies. How is this testing performed, and what are
some factors that could lead to these having errors during manufacture.

2. Every now and then, high impact cpu bugs are discovered that threaten the security or
dependability of cpu's. How are these bugs usually found, and how can manufacturers improve on
their testing methods to more reliably find these themselves before shipping.
```

### 1.8-1.13
```
1. The book mentions multiple issues with various performance specs but continues to use
them. Nowadays there are projects like https://google.github.io/fuzzbench/ available that span
thousands of fairly well distributed open source projects. Would something like that not be better?

2. These chapters talk a good bit about fine grained performance tracking (how much execution
time is spent in common instructions, etc). How are these numbers usually obtained, and how
would a cpu-research environment usually look. Is it generally just based on Simulators like Bochs?
```

### B.1-B.3
```
1. The book briefly mentions that larger caches result in longer hit-times alongside higher
   power/energy consumption. The latter makes sense, but why would hit-times be longer? Assuming the
   same speed cache is used (obviously this gets a lot more expensive with a larger cache) and the
   associativity isn't increased, a cache-search just indexes an array. Why does a larger sized
   cache potentially slow down this access?

2. Why does the first-level cache affect the clock-rate of the processor while the second level does
   not?
```

### 2.1-2.3
```
1. Why are write buffers used? Does that not duplicate memory from the cache & the write buffer? Would 
   it not be easier if a list of cache-sets to be written back was simply maintained?

2. How do non-blocking caches provide such large benefits? Let's assume an L1-miss-penalty of 10. For this
   optimization to be helpful, would it not be required that the processor not operate on the requested data
   for at least 10 cycles? If it does it would have to stall anyways until the data finally comes in. 
   With	modern processors frequently executing 2-4 instructions per cycle, this seems like an unlikely edge
   case rather than the norm.
```

### C
